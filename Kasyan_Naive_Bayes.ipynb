{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dGL2PoxJiGok",
      "metadata": {
        "id": "dGL2PoxJiGok"
      },
      "source": [
        "# Naive Bayes\n",
        "Вам необходимо реализовать наивный байесовский классификатор для анализа тональности твитов. Анализ тональности, в нашем случае, это простая задача бинарной классификации, в которой вам предстоит определить явяляется ли твит позитивным (по настроению) или негативным.\n",
        "\n",
        "Этапы решения задачи:\n",
        "\n",
        "* Обучить модель наивного байесовского классификатора для определения тональности текста.\n",
        "* Протестировать модель на валидационном наборе данных.\n",
        "* Вычислить отношение позитивных слов к негативным.\n",
        "* Проанализировать ошибки.\n",
        "* Протестировать модель на новых твитах."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H1a_V100dN8k",
      "metadata": {
        "id": "H1a_V100dN8k"
      },
      "source": [
        "В данном задании приведены некоторые шаблоны функций, которые вам нужно реализовать, однако, если вам не нравится их нотация, либо вы хотите выполнить задание по-другому, можете не использовать эти шаблоны."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qJuyJfMEw6wD",
      "metadata": {
        "id": "qJuyJfMEw6wD"
      },
      "source": [
        "```nltk``` - это библиотека, которая содержит ряд инструментов по работе с текстовыми данными. \n",
        "\n",
        "Например, с помощью нее можно загружать стандартные датасеты (например, тексты твитов для классификации), список стоп-слов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 312,
      "id": "0cd0efd7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Looking in indexes: https://mirrors.sustech.edu.cn/pypi/simple\n",
            "Requirement already satisfied: nltk in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (3.8)\n",
            "Requirement already satisfied: joblib in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: tqdm in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: click in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (from nltk) (8.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\leo\\appdata\\roaming\\python\\python310\\site-packages (from click->nltk) (0.4.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "id": "1DYIB3oaiGon",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DYIB3oaiGon",
        "outputId": "5288a5d7-37d1-491d-e466-006e2908cfa8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to\n",
            "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Leo\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 313,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# загружаем необходимые библиотеки\n",
        "import pdb\n",
        "from nltk.corpus import stopwords, twitter_samples\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "import string\n",
        "from os import getcwd\n",
        "\n",
        "# Загружаем стандартный набор твитов из библиотеки nltk \n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "id": "zuPDbUsMJbUr",
      "metadata": {
        "id": "zuPDbUsMJbUr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "import numpy as np "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "id": "ytggoE5W3p4Q",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ytggoE5W3p4Q",
        "outputId": "a468ed25-89c5-4fd1-977f-72f0ab365712"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\Leo\\\\Desktop\\\\Misis\\\\ML_1_sem\\\\Project_3'"
            ]
          },
          "execution_count": 315,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 316,
      "id": "6JpPL_99iGop",
      "metadata": {
        "id": "6JpPL_99iGop"
      },
      "outputs": [],
      "source": [
        "# Получаем доступ к директории, в которой хранятся твиты\n",
        "filePath = f\"{getcwd()}/../tmp2/\"\n",
        "nltk.data.path.append(filePath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 317,
      "id": "kqugMVFoiGop",
      "metadata": {
        "id": "kqugMVFoiGop"
      },
      "outputs": [],
      "source": [
        "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "\n",
        "# стандартный сплит данных на test и train\n",
        "# можете реализовать с помощью train_test_split, если посчитаете нужным\n",
        "test_pos = all_positive_tweets[4000:]\n",
        "train_pos = all_positive_tweets[:4000]\n",
        "test_neg = all_negative_tweets[4000:]\n",
        "train_neg = all_negative_tweets[:4000]\n",
        "\n",
        "train_x = train_pos + train_neg\n",
        "test_x = test_pos + test_neg\n",
        "\n",
        "# приписываем соответствующие метки классов\n",
        "# если делали деление с помощью train_test_split, лучше добавить метки классов сразу к исходным данным\n",
        "train_y = np.append(np.ones(len(train_pos)), np.zeros(len(train_neg)))\n",
        "test_y = np.append(np.ones(len(test_pos)), np.zeros(len(test_neg)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UUz6twqtiGoq",
      "metadata": {
        "id": "UUz6twqtiGoq"
      },
      "source": [
        "# Часть 1: Предобработка данных"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gzpCf0XdyjMv",
      "metadata": {
        "id": "gzpCf0XdyjMv"
      },
      "source": [
        "Итак, для построения наивного байесовского классификатора каждый текст должен быть описан списком релевантных токенов.\n",
        "\n",
        "Токены - это смысловая часть слова, которая может часто встречатться в языке. Для того чтобы выделить токены из текста воспользуемся классом ```TweetTokenizer``` библиотеки nltk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 318,
      "id": "h5_1gtEo0qrS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5_1gtEo0qrS",
        "outputId": "6834b40e-3134-474f-ae80-e17538116d09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['i', 'like', 'machine', 'learning']\n"
          ]
        }
      ],
      "source": [
        "tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    \n",
        "input_text = \"I like Machine Learning\"\n",
        "text = tokenizer.tokenize(input_text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uxsFxXCK18m4",
      "metadata": {
        "id": "uxsFxXCK18m4"
      },
      "source": [
        "Однако чтобы находить сходство между текстами, необходимо, чтобы разные формы слов (время, падеж, суффиксы, приставки) воспринимались как один токен, например, *like* и *likes*.\n",
        "\n",
        "Для этого нужно воспользоваться стеммингом. Стемминг - процесс приведения слова к его стемме (по сути, к корню).\n",
        "\n",
        "В библиотеке ```nltk``` реализован класс [```PorterStemmer```](https://www.nltk.org/howto/stem.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 319,
      "id": "57Sgam7Z17qa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57Sgam7Z17qa",
        "outputId": "910b0753-7f5b-4484-e089-07f5d1bc88b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "like\n",
            "appl appl\n"
          ]
        }
      ],
      "source": [
        "stemmer = PorterStemmer()\n",
        "print(stemmer.stem('likes'))\n",
        "print(stemmer.stem('apple'), stemmer.stem('apples'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WE1ruHkZ33bI",
      "metadata": {
        "id": "WE1ruHkZ33bI"
      },
      "source": [
        "Также при обработке текстовых данных часто требуется удалять лишние символы. В некоторых задачах это могут быть знаки препинания, служебные символы, гиперссылки и т.д. Они не несут в себе семантической нагрузки и могут помешать классификатору в определении класса текста.\n",
        "\n",
        "Удаление лишних символов можно производить с помощью регулярных выражений модуля ```re```.\n",
        "\n",
        "Например, чтобы удалить знаки пунктуации из строки, можно выполнить следующую функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 320,
      "id": "ST-MHBw46ItK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST-MHBw46ItK",
        "outputId": "d2e580c3-a32b-42b9-d113-0350351ea12e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello What a nice day\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello! What a nice day!!!\"\n",
        "text = re.sub(r'[.,\"\\'-?:!;]', '', text)\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XHsrPKTc32US",
      "metadata": {
        "id": "XHsrPKTc32US"
      },
      "source": [
        "### Задание 1.\n",
        "\n",
        "Проведите предобработку данных:\n",
        "- Удаление шума: удаляем слова, которые не имеют значения для решаемой задачи. Предположительно, это должны быть такие общие слова как 'I, you, are, is, etc...'.\n",
        "- Также необходио удалить служебные символы, гиперссылки, хэштеги и т.д.\n",
        "- Знаки пунктуации (опционально), во-первых стоит удалить знаки пунктуации, если они нарушают выделение уникального слова, например, \"happy\", \"happy?\", \"happy!\", \"happy,\", значимое слово здесь \"happy\", нам необходимо выделить его как единый токен. Однако, вы можете попробовать выделить в осмысленные токены значимую комбинацию знаков пунктуации, например, ':-)', ')', '(' и т.д.\n",
        "- Произвести стемминг, привести слова к нормальной форме. Например, такие слова как \"motivation\", \"motivated\", and \"motivate\" будут представлены одной формой \"motiv\".\n",
        "\n",
        "Реализуйте функцию `process_tweet` для проведения предобработки твитов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 321,
      "id": "vxNgNOXeJVOR",
      "metadata": {
        "id": "vxNgNOXeJVOR"
      },
      "outputs": [],
      "source": [
        "def process_tweet(tweet):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: a string containing a tweet\n",
        "    Output:\n",
        "        tweets_clean: a list of words containing the processed tweet\n",
        "\n",
        "    '''\n",
        "    tweet_new = ''\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "\n",
        "    \n",
        "    # удаления символов типа $GE\n",
        "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
        "\n",
        "    \n",
        "    # удалени символа ретвита \"RT\"\n",
        "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
        "\n",
        "    # удалите гиперссылки\n",
        "    tweet = re.sub(r\"https?://[^,\\s]+,?\", \"\", tweet)\n",
        "    # поиск смайлика\n",
        "    smiley = re.findall(r'(?:\\<3|\\(?\\^\\^\\)?|[-^]?[@:;][oO]?[D\\)\\]\\(\\]/\\\\OpP])', tweet)\n",
        "\n",
        "    # удалите символ '#' для того, чтобы работать с хэштегами как с обычными словами\n",
        "    tweet = re.sub(r'\\#', '', tweet)\n",
        "    tweet = re.sub(r\"@\\S+\", '', tweet)\n",
        "    tweet =re.sub(r'[^a-zA-Z0-9\\s^|]', '', tweet) \n",
        "    tweet = re.sub(r'[.,\"\\'-?:!;]', '', tweet)\n",
        "    \n",
        "    # токенизация твитов\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
        "                               reduce_len=True)\n",
        "    \n",
        "    tweet_tokens = tokenizer.tokenize(tweet)\n",
        "\n",
        "    temp_list = [stemmer.stem(word) for word in tweet_tokens if word.lower() not in stopwords_english]\n",
        "    if len(smiley) >0:\n",
        "        for i in smiley:\n",
        "            temp_list.append(i)\n",
        "    tweets_clean = []\n",
        "    # токенизируйте текст и приведите слова к нормальной форме\n",
        "    \n",
        "    for word in temp_list:\n",
        "         tweets_clean.append(word)\n",
        "\n",
        "    return tweets_clean\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 322,
      "id": "U2UK-FPaiGoq",
      "metadata": {
        "id": "U2UK-FPaiGoq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hello', 'great', 'day', 'good', 'morn', ':)']\n"
          ]
        }
      ],
      "source": [
        "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
        "\n",
        "# проверьте функцию для предобработки твитов\n",
        "print(process_tweet(custom_tweet))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6EFMA-LF_tGf",
      "metadata": {
        "id": "6EFMA-LF_tGf"
      },
      "source": [
        "## Часть 1.1 Реализация вспомогательных функций\n",
        "\n",
        "Чтобы рассчитать $P(w_i|class)$ необходимо усеть рассчитывать частоту $(w_i|class)$ для всех текстов из обучающей выборки.\n",
        "\n",
        "Необходимо завести словарь, где ключом будет являться кортеж (слово, класс), а значения, соответствующие этому кортежу, частотой в обучающем корпусе. Класс 1 указывает позитивную тональность, 0 - на негативную.\n",
        "\n",
        "Необходимо реализовать вспомогательную функцию поиска, которая принимает словарь `freqs`, слово и класс (1 или 0) и возвращает количество раз, которое кортеж (слово, класс) встречались в корпусе.\n",
        "\n",
        "Например, для списка твитов `[\"i am rather excited\", \"you are rather happy\"]`, соответствующих классу 1, функция вернет словарь:\n",
        "\n",
        "{\n",
        "    (\"rather\", 1): 2,\n",
        "    (\"happi\", 1) : 1, \n",
        "    (\"excit\", 1) : 1\n",
        "}\n",
        "\n",
        "#### Задания\n",
        "Реализуйте функцию `count_tweets`, которая получает на вход список твитов, производит их предобработку и взвращает соответствующий словарь.\n",
        "- Ключом словаря является кортеж, содержащий нормальную форму слова и метку класса word, например, (\"happi\",1).\n",
        "- Значение - сколько раз это слово появилось в текстах соответствующих классов (integer)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 323,
      "id": "fk49BUeHiGos",
      "metadata": {
        "id": "fk49BUeHiGos"
      },
      "outputs": [],
      "source": [
        "def count_tweets(tweets, ys):\n",
        "    '''\n",
        "    Input:\n",
        "        tweets: список твитов\n",
        "        ys: класс твита (0 или 1)\n",
        "    Output:\n",
        "        result: словарь, сопоставляющий пары (слово, класс) частоте\n",
        "    '''\n",
        "    result = {}\n",
        "    map_count = {}\n",
        "    for i, words in enumerate(tweets):\n",
        "        temp_list = process_tweet(words)\n",
        "        for word in temp_list:\n",
        "            result_tuple = (word, ys[i])\n",
        "            # Добавляем новый ключ в словарь со значением 1,\n",
        "            # если слово не было использовано с этим классом\n",
        "            if result.get(result_tuple, None) is None:\n",
        "                result[(word, ys[i])] = 1\n",
        "            else:\n",
        "                result[(word, ys[i])] += 1\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 324,
      "id": "e6S3_6BTiGot",
      "metadata": {
        "id": "e6S3_6BTiGot"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
            ]
          },
          "execution_count": 324,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Тестирование функции\n",
        "\n",
        "tweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\n",
        "ys = [1, 0, 0, 0, 0]\n",
        "result = count_tweets(tweets, ys)\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iF6PSI_siGot",
      "metadata": {
        "id": "iF6PSI_siGot"
      },
      "source": [
        "**Ожидаемый результат**: {('happi', 1): 1, ('trick', 0): 1, ('sad', 0): 1, ('tire', 0): 2}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deOWdCJIiGou",
      "metadata": {
        "id": "deOWdCJIiGou"
      },
      "source": [
        "# Часть 2: Обучите наивный байесовский классификатор\n",
        "\n",
        "Наивный Байес - это алгоритм, который можно использовать для анализа тональности текстов. Алгоритм быстрый, не требует много времени для обучения.\n",
        "\n",
        "#### Процесс обучения наивного байесовского классификатора\n",
        "- Определить число классов.\n",
        "- Рассчитать вероятность для каждого класса.\n",
        "$P(D_{pos})$ - вероятность увидеть документ с положительной тональностью.\n",
        "$P(D_{отрицание})$ - вероятность увидеть документ с отрицательной тональностью.\n",
        "\n",
        "$$P(D_{pos}) = \\frac{D_{pos}}{D}\\tag{1}$$\n",
        "\n",
        "$$P(D_{neg}) = \\frac{D_{neg}}{D}\\tag{2}$$\n",
        "\n",
        "где $D$ - общее количество документов, или твитов в данном случае, $D_{pos}$ - общее количество положительных твитов, а $D_{neg}$ - общее количество отрицательных твитов."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "O0qU1y-NiGou",
      "metadata": {
        "id": "O0qU1y-NiGou"
      },
      "source": [
        "#### Prior and Logprior\n",
        "\n",
        "Априорная вероятность (Prior) - вероятность того, что мы можем получит позитивный твит в нашем корусе документов. Prior определяется как отношение вероятностей $\\frac{P(D_{pos})}{P(D_{neg})}$.\n",
        "\n",
        "Для удобства можем рассчитать логарифм этой вероятности - logprior:\n",
        "\n",
        "$\\text{logprior} = log \\left( \\frac{P(D_{pos})}{P(D_{neg})} \\right) = log \\left( \\frac{D_{pos}}{D_{neg}} \\right)$.\n",
        "\n",
        "Так как $log(\\frac{A}{B}) = log(A) - log(B)$:\n",
        "\n",
        "$\\text{logprior} = \\log (P(D_{pos})) - \\log (P(D_{neg})) = \\log (D_{pos}) - \\log (D_{neg})\\tag{3}$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_jrJ6B_biGou",
      "metadata": {
        "id": "_jrJ6B_biGou"
      },
      "source": [
        "#### Положительная и отрицательная вероятность слова\n",
        "\n",
        "Чтобы вычислить положительную вероятность и отрицательную вероятность для определенного слова в словаре, мы будем использовать следующие входные данные:\n",
        "\n",
        "- $freq_{pos}$ и $freq_{neg}$ являются частотами этого конкретного слова в положительном или отрицательном классе. Другими словами, положительная частота слова - это количество раз, когда слово подсчитывается с меткой 1.\n",
        "- $N_{pos}$ и $N_{neg}$ - общее количество положительных и отрицательных слов для всех документов (для всех твитов) соответственно.\n",
        "- $V$ - это количество уникальных слов во всем наборе документов для всех классов, как положительных, так и отрицательных.\n",
        "\n",
        "Мы будем использовать их для вычисления положительной и отрицательной вероятности для определенного слова, используя эту формулу:\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "Здесь реализуется сглаживание Лапласа. [Здесь](https://en.wikipedia.org/wiki/Additive_smoothing) можно прочитать про процедуру сглаживания."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QMib8wX2iGov",
      "metadata": {
        "id": "QMib8wX2iGov"
      },
      "source": [
        "#### Log likelihood\n",
        "Для вычисления loglikelihood можно воспользоваться следующей формулой:\n",
        "\n",
        "$$\\text{loglikelihood} = \\log \\left(\\frac{P(W_{pos})}{P(W_{neg})} \\right)\\tag{6}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZSQwNhFAiGov",
      "metadata": {
        "id": "ZSQwNhFAiGov"
      },
      "source": [
        "##### Теперь нужно создать словарь частот (`freqs`)\n",
        "- Применяя функцию `count_tweets`, можно вычислить словарь `freqs`, который содержит все частоты.\n",
        "- В словаре `freqs` ключом является кортеж (слово, *класс*).\n",
        "- Значение - частота появления слова в текстах соответствующих *классу*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 325,
      "id": "D5ziufr1iGov",
      "metadata": {
        "id": "D5ziufr1iGov"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "10733"
            ]
          },
          "execution_count": 325,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Построение словаря частот\n",
        "freqs = count_tweets(train_x, train_y)\n",
        "len(freqs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P6UuqnAOiGov",
      "metadata": {
        "id": "P6UuqnAOiGov"
      },
      "source": [
        "#### Задания\n",
        "\n",
        "Дано: словарь частот, `train_x` (список твитов) и `train_y` (список классов, характеризующих тональность твита). Необходимо реализовать наивный байесовский классификатор.\n",
        "\n",
        "##### Вычисление $V$\n",
        "- Необходимо рассчитать количество уникальных слов в словаре `freqs`.\n",
        "\n",
        "##### Вычисление $freq_{pos}$ и $freq_{neg}$\n",
        "- Используя словарь `freqs`, можно вычислить частоты для позитивных и негативных элементов: $freq_{pos}$ и $freq_{neg}$.\n",
        "\n",
        "##### Вычисление $N_{pos}$ и $N_{neg}$\n",
        "- Используя словарь `freqs`, можно также вычислить общее количество слов в позитивных и негативных документах: $N_{pos}$ и $N_{neg}$.\n",
        "\n",
        "##### Вычисление $D$, $D_{pos}$, $D_{neg}$\n",
        "- Используя входной список классов `train_y`, вычислите общее количество документов $D$, а также количество позитивных и негативных документов: $D_{pos}$ и $D_{neg}$.\n",
        "- Вычислите вероятность того, что твит имеет положительную тональность $P(D_{pos})$, а также вероятность, что твит имеет отрицательную тональность $P(D_{neg})$.\n",
        "\n",
        "##### Вычислите logprior\n",
        "- logprior = $log(D_{pos}) - log(D_{neg})$\n",
        "\n",
        "##### Вычислите log likelihood\n",
        "- В заключение нужно пройти по всем словам в словаре, для этого нужно использовать функцию `lookup` для получения положительных и отрицательных частот: $freq_{pos}$ и $freq_{neg}$ для каждого слова.\n",
        "- Вычислите $P(W_{pos})$ и $P(W_{neg})$ используя формулы 4 и 5.\n",
        "\n",
        "$$ P(W_{pos}) = \\frac{freq_{pos} + 1}{N_{pos} + V}\\tag{4} $$\n",
        "$$ P(W_{neg}) = \\frac{freq_{neg} + 1}{N_{neg} + V}\\tag{5} $$\n",
        "\n",
        "- Далее можно вычислить loglikelihood: $log \\left( \\frac{P(W_{pos})}{P(W_{neg})} \\right)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 326,
      "id": "snRcQyqPJltA",
      "metadata": {
        "id": "snRcQyqPJltA"
      },
      "outputs": [],
      "source": [
        "def lookup(freqs, word, label):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: словарь с частотами для каждой пары\n",
        "        word: слово, которое нужно найти\n",
        "        label: класс, соответствующий слову\n",
        "    Output:\n",
        "        n: количество раз, которое слово и соответствующая ему метка появились в корпусе.\n",
        "    '''\n",
        "    n = 0 \n",
        "\n",
        "    pair = (word, label)\n",
        "    if (pair in freqs):\n",
        "        n = freqs[pair]\n",
        "\n",
        "    return n\n",
        "\n",
        "def test_lookup(func):\n",
        "    freqs = {('sad', 0): 4,\n",
        "             ('happy', 1): 12,\n",
        "             ('oppressed', 0): 7}\n",
        "    word = 'happy'\n",
        "    label = 1\n",
        "    if func(freqs, word, label) == 12:\n",
        "        return 'SUCCESS!!'\n",
        "    return 'Failed Sanity Check!'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b446e834",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 327,
      "id": "g-XXaduQiGow",
      "metadata": {
        "id": "g-XXaduQiGow"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "def train_naive_bayes(freqs, train_x, train_y):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: словарь: {(слово, класс), частота}\n",
        "        train_x: список твитов\n",
        "        train_y: список классов (0,1), соответствующих твитам\n",
        "    Output:\n",
        "        logprior: log prior (формула 3).\n",
        "        loglikelihood: log likelihood (формула 6).\n",
        "    '''\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "\n",
        "    # подсчет уникальных значений\n",
        "    # подсчет n_pos and n_neg\n",
        "    uniq = set()# уникальные значения слов\n",
        "    uniq_pos = set() \n",
        "    uniq_neg = set()\n",
        "    for word in freqs:\n",
        "        if word[1]==1:\n",
        "            uniq_pos.add(word[0])\n",
        "        else:\n",
        "            uniq_neg.add(word[0])\n",
        "        uniq.add(word[0])\n",
        "    n_pos = len(uniq_pos)\n",
        "    n_neg = len(uniq_neg)\n",
        "    uniq_count = len(uniq) # кол-во уникальных значений\n",
        "\n",
        "    # подсчет d, d_neg, d_pos\n",
        "    d = len(train_y)\n",
        "    d_pos = len(train_y[train_y == 1])\n",
        "    d_neg = len(train_y[train_y == 0])\n",
        "\n",
        "    # подсчет p_d_neg, p_d_pos\n",
        "    p_d_neg = d_neg / d \n",
        "    p_d_pos = d_pos / d\n",
        "\n",
        "\n",
        "    # подсчет logprior\n",
        "    logpror = math.log(d_pos) - math.log(d_neg)\n",
        "\n",
        "    # подсчет log likelihood\n",
        "    for word in uniq:\n",
        "        freq_pos = lookup(freqs, word, 1)\n",
        "        freq_neg = lookup(freqs, word, 0)\n",
        "        p_w_pos = (freq_pos + 1) / (n_pos + uniq_count)\n",
        "        p_w_neg = (freq_neg + 1) / (n_neg + uniq_count)\n",
        "        loglikelihood[word] = math.log(p_w_pos) - math.log(p_w_neg)\n",
        "\n",
        "    return logprior, loglikelihood\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 328,
      "id": "qXCIEpYIiGow",
      "metadata": {
        "id": "qXCIEpYIiGow"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "8563\n"
          ]
        }
      ],
      "source": [
        "logprior, loglikelihood = train_naive_bayes(freqs, train_x, train_y)\n",
        "print(logprior)\n",
        "print(len(loglikelihood))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DkzatLZviGow",
      "metadata": {
        "id": "DkzatLZviGow"
      },
      "source": [
        "**Ожидаемый результат**:\n",
        "\n",
        "0.0\n",
        "\n",
        "91.65"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LpfQ60riQSgn",
      "metadata": {
        "id": "LpfQ60riQSgn"
      },
      "source": [
        "# Часть 3: Тестирование классификатора"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ej3dSSdVALlD",
      "metadata": {
        "id": "Ej3dSSdVALlD"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "bSNsknCMAJyQ",
      "metadata": {
        "id": "bSNsknCMAJyQ"
      },
      "source": [
        "Сделаем прогноз на тестовых твитах.\n",
        "\n",
        "#### Реализовать функцию `naive_bayes_predict`\n",
        "Задания:\n",
        "\n",
        "Реализуйте функцию `naive_bayes_predict`, чтобы делать прогнозы в твитах.\n",
        "\n",
        "* Функция принимает `tweet`, `logprior`, `loglikelihood`.\n",
        "* Функция возвращает вероятность того, что твит относится к позитивному или негативному классу.\n",
        "\n",
        "$$ p = logprior + \\sum_i^N (loglikelihood_i)$$\n",
        "\n",
        "#### Примечание\n",
        "Обратите внимание, сбалансирован ли набор данных, с которым вы работаете в этом задании, и чему будет равен logprior."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 329,
      "id": "xttyzvbfQSgn",
      "metadata": {
        "id": "xttyzvbfQSgn"
      },
      "outputs": [],
      "source": [
        "def naive_bayes_predict(tweet, logprior, loglikelihood):\n",
        "    '''\n",
        "    Input:\n",
        "        tweet: строка\n",
        "        logprior: число\n",
        "        loglikelihood: словарь, переводящий слово в соответствующий ему logprior\n",
        "    Output:\n",
        "        p: сумма всех logliklihoods для каждого слова из твита (если оно содержится в словаре) + logprior (число)\n",
        "\n",
        "    '''\n",
        "    word_l = 0\n",
        "    words = process_tweet(tweet)\n",
        "    p = logprior + sum(loglikelihood.get(stem, 0) for stem in words)\n",
        "    return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 330,
      "id": "qNH6w_8lQSgn",
      "metadata": {
        "id": "qNH6w_8lQSgn"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The expected output is 1.5367220996043542\n"
          ]
        }
      ],
      "source": [
        "my_tweet = 'She smiled.'\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print('The expected output is', p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "H5pBQIZKiGox",
      "metadata": {
        "id": "H5pBQIZKiGox"
      },
      "source": [
        "**Ожидаемый результат**:\n",
        "- Ожидаемый результат - около 1.55\n",
        "- Тональность позитивная."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wW9S9r6uiGoy",
      "metadata": {
        "id": "wW9S9r6uiGoy"
      },
      "source": [
        "#### Реализуйте функцию test_naive_bayes\n",
        "**Описание функции**:\n",
        "* Функция `test_naive_bayes` оценивает точность предсказаний.\n",
        "* Функция принимает на вход `test_x`, `test_y`, log_prior и loglikelihood\n",
        "* Функция возвращает точность для тестовой выборки."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 331,
      "id": "ig9W8hoKiGoy",
      "metadata": {
        "id": "ig9W8hoKiGoy"
      },
      "outputs": [],
      "source": [
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood, naive_bayes_predict=naive_bayes_predict):\n",
        "    \"\"\"\n",
        "    Input:\n",
        "        test_x: список твитов\n",
        "        test_y: соответствующие классы для твитов\n",
        "        logprior: logprior\n",
        "        loglikelihood: словарь с loglikelihoods для каждого слова\n",
        "    Output:\n",
        "        accuracy: точность предсказания\n",
        "    \"\"\"\n",
        "    accuracy = 0 \n",
        "\n",
        "    pred_y = np.array(\n",
        "        [naive_bayes_predict(tweet, logprior, loglikelihood) > 0 for tweet in test_x]\n",
        "    )\n",
        "\n",
        "    error = np.abs(test_y - pred_y).mean()\n",
        "    accuracy = 1 - error\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 332,
      "id": "ZenXehTciGoy",
      "metadata": {
        "id": "ZenXehTciGoy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes accuracy = 0.9595\n"
          ]
        }
      ],
      "source": [
        "print(\"Naive Bayes accuracy = %0.4f\" %\n",
        "      (test_naive_bayes(test_x, test_y, logprior, loglikelihood)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5ff3c0bb",
      "metadata": {},
      "source": [
        "Не понимаю как достичь такой точности("
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UmSqfBP8iGoy",
      "metadata": {
        "id": "UmSqfBP8iGoy"
      },
      "source": [
        "**Ожидаемая точность**:\n",
        "\n",
        "`Точность наивного байесовского классификатора = 0.9955`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 333,
      "id": "b2zg72yWiGoy",
      "metadata": {
        "id": "b2zg72yWiGoy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am happy -> 2.13\n",
            "I am bad -> -1.19\n",
            "this movie should have been great. -> 2.06\n",
            "great -> 2.13\n",
            "great great -> 4.26\n",
            "great great great -> 6.39\n",
            "great great great great -> 8.52\n"
          ]
        }
      ],
      "source": [
        "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:    \n",
        "    p = naive_bayes_predict(tweet, logprior, loglikelihood)\n",
        "    print(f'{tweet} -> {p:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pnVCInvhiGoz",
      "metadata": {
        "id": "pnVCInvhiGoz"
      },
      "source": [
        "**Ожидаемый результат**:\n",
        "- I am happy -> 2.14\n",
        "- I am bad -> -1.31\n",
        "- this movie should have been great. -> 2.12\n",
        "- great -> 2.13\n",
        "- great great -> 4.26\n",
        "- great great great -> 6.39\n",
        "- great great great great -> 8.52"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 334,
      "id": "4yOYkyDWiGoz",
      "metadata": {
        "id": "4yOYkyDWiGoz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "-8.734857373068746"
            ]
          },
          "execution_count": 334,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Проверьте тональность на любом твите\n",
        "my_tweet = 'you are bad :('\n",
        "naive_bayes_predict(my_tweet, logprior, loglikelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uGPW8LdXiGoz",
      "metadata": {
        "id": "uGPW8LdXiGoz"
      },
      "source": [
        "# Часть 4: Отфильтруйте слова по соотношению положительных и отрицательных значений\n",
        "\n",
        "- Некоторые слова чаще втречаются в положительном контексте, чем другие, и их можно считать \"более позитивными\". Аналогично, другие слова можно считать \"более негативными\".\n",
        "- Один из способов определить уровень позитивности или негативности, не вычисляя логарифмическую вероятность, - сравнить положительную и отрицательную частоту слова.\n",
        "- Можно использовать расчеты логарифмического правдоподобия (log likelihood) для сравнения относительной позитивности или негативности слов.\n",
        "- Для каждого слова можно рассчитать соотношение положительных и отрицательных частот этого слова.\n",
        "- Слова можно отфильтровать в соответствии с рассчитанными соотношениями.\n",
        "\n",
        "#### Реализовать функцию get_ratio\n",
        "\n",
        "$$ ratio = \\frac{\\text{pos_words} + 1}{\\text{neg_words} + 1} $$\n",
        "\n",
        "где pos_words и neg_words сответствуют частотам слов по классам. \n",
        "<table>\n",
        "    <tr>\n",
        "        <td>\n",
        "            <b>Words</b>\n",
        "        </td>\n",
        "        <td>\n",
        "        Positive word count\n",
        "        </td>\n",
        "         <td>\n",
        "        Negative Word Count\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        glad\n",
        "        </td>\n",
        "         <td>\n",
        "        41\n",
        "        </td>\n",
        "    <td>\n",
        "        2\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        arriv\n",
        "        </td>\n",
        "         <td>\n",
        "        57\n",
        "        </td>\n",
        "    <td>\n",
        "        4\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        :(\n",
        "        </td>\n",
        "         <td>\n",
        "        1\n",
        "        </td>\n",
        "    <td>\n",
        "        3663\n",
        "        </td>\n",
        "  </tr>\n",
        "    <tr>\n",
        "        <td>\n",
        "        :-(\n",
        "        </td>\n",
        "         <td>\n",
        "        0\n",
        "        </td>\n",
        "    <td>\n",
        "        378\n",
        "        </td>\n",
        "  </tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 335,
      "id": "GnHfpbQjiGoz",
      "metadata": {
        "id": "GnHfpbQjiGoz"
      },
      "outputs": [],
      "source": [
        "def get_ratio(freqs, word):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: словарь, соодержащий слова\n",
        "\n",
        "    Output: словарь с ключами: 'positive', 'negative' и 'ratio'.\n",
        "        Пример: {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "    '''\n",
        "    pos_neg_ratio = {'positive': 0, 'negative': 0, 'ratio': 0.0}\n",
        "    pos_neg_ratio[\"positive\"] = freqs.get((word, 1), 0)\n",
        "    pos_neg_ratio[\"negative\"] = freqs.get((word, 0), 0)\n",
        "    pos_neg_ratio[\"ratio\"] = (pos_neg_ratio[\"positive\"] + 1) / (pos_neg_ratio[\"negative\"] + 1)\n",
        "   \n",
        "    return pos_neg_ratio\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 336,
      "id": "C0ntGhFLiGoz",
      "metadata": {
        "id": "C0ntGhFLiGoz"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'positive': 161, 'negative': 18, 'ratio': 8.526315789473685}"
            ]
          },
          "execution_count": 336,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_ratio(freqs, 'happi')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8aowQUFwiGo0",
      "metadata": {
        "id": "8aowQUFwiGo0"
      },
      "source": [
        "#### Реализуйте функцию get_words_by_threshold(freqs,label,threshold)\n",
        "\n",
        "* Используйте функцию `get_ratio` для составления словаря, состоящего из 'положительного' и 'отрицательного' количества, а также ratio для этих значений.\n",
        "* Добавьте составленный словарь `get_ratio` в качестве значения для другого словаря, в котором ключом является само слово.\n",
        "\n",
        "Пример элемента словаря:\n",
        "```\n",
        "{'happi':\n",
        "    {'positive': 10, 'negative': 20, 'ratio': 0.524}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 337,
      "id": "UQzpe-LoiGo0",
      "metadata": {
        "id": "UQzpe-LoiGo0"
      },
      "outputs": [],
      "source": [
        "def get_words_by_threshold(freqs, label, threshold, get_ratio=get_ratio):\n",
        "    '''\n",
        "    Input:\n",
        "        freqs: словарь слов\n",
        "        label: 1, 0\n",
        "        threshold: ratio - порог, по которому мы будем решать, включать ли слово в словарь\n",
        "    Output:\n",
        "        word_list: словарь, содержащий слово и информацию о его \"позитивном количестве\", \"негативном количестве\", и отношении (ratio) \"позитивного количества\" к негативному.\n",
        "        Пример:\n",
        "        {'happi':\n",
        "            {'positive': 10, 'negative': 20, 'ratio': 0.5}\n",
        "        }\n",
        "    '''\n",
        "    word_list = {}\n",
        "    for word in freqs:\n",
        "        pos_neg_ratio = get_ratio(freqs, word[0])\n",
        "        if ((label == 1) and (pos_neg_ratio[\"ratio\"] >= threshold)) or (\n",
        "            (label == 0) and (pos_neg_ratio[\"ratio\"] <= threshold)):\n",
        "            word_list[word[0]] = pos_neg_ratio\n",
        "    \n",
        "    return word_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 338,
      "id": "V7JsWpGtiGo0",
      "metadata": {
        "id": "V7JsWpGtiGo0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{':(': {'positive': 1, 'negative': 3723, 'ratio': 0.0005370569280343716},\n",
              " 'zayniscomingbackonjuli': {'positive': 0, 'negative': 19, 'ratio': 0.05},\n",
              " 'belev': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'wll': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776},\n",
              " 'justn': {'positive': 0, 'negative': 35, 'ratio': 0.027777777777777776}}"
            ]
          },
          "execution_count": 338,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_words_by_threshold(freqs, label=0, threshold=0.05)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 339,
      "id": "t65gzv6OiGo0",
      "metadata": {
        "id": "t65gzv6OiGo0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'followfriday': {'positive': 23, 'negative': 0, 'ratio': 24.0},\n",
              " 'commun': {'positive': 27, 'negative': 1, 'ratio': 14.0},\n",
              " ':)': {'positive': 2966, 'negative': 4, 'ratio': 593.4},\n",
              " 'flipkartfashionfriday': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " ':D': {'positive': 529, 'negative': 0, 'ratio': 530.0},\n",
              " 'p': {'positive': 107, 'negative': 3, 'ratio': 27.0},\n",
              " ':p': {'positive': 104, 'negative': 0, 'ratio': 105.0},\n",
              " 'influenc': {'positive': 16, 'negative': 0, 'ratio': 17.0},\n",
              " 'here': {'positive': 20, 'negative': 0, 'ratio': 21.0},\n",
              " 'youth': {'positive': 15, 'negative': 0, 'ratio': 16.0},\n",
              " 'bam': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'warsaw': {'positive': 44, 'negative': 0, 'ratio': 45.0},\n",
              " 'shout': {'positive': 11, 'negative': 0, 'ratio': 12.0},\n",
              " ';)': {'positive': 22, 'negative': 0, 'ratio': 23.0},\n",
              " 'stat': {'positive': 51, 'negative': 0, 'ratio': 52.0},\n",
              " 'arriv': {'positive': 57, 'negative': 4, 'ratio': 11.6},\n",
              " 'glad': {'positive': 41, 'negative': 2, 'ratio': 14.0},\n",
              " 'blog': {'positive': 27, 'negative': 0, 'ratio': 28.0},\n",
              " 'fav': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
              " 'fantast': {'positive': 9, 'negative': 0, 'ratio': 10.0},\n",
              " 'fback': {'positive': 26, 'negative': 0, 'ratio': 27.0},\n",
              " 'pleasur': {'positive': 10, 'negative': 0, 'ratio': 11.0},\n",
              " 'aqui': {'positive': 9, 'negative': 0, 'ratio': 10.0}}"
            ]
          },
          "execution_count": 339,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_words_by_threshold(freqs, label=1, threshold=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "td7vpiQ_iGo0",
      "metadata": {
        "id": "td7vpiQ_iGo0"
      },
      "source": [
        "Проанализируйте, насколько интуитивно были определены \"позитивные\" и \"негативные\" слова."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r9oI-KbRiGo1",
      "metadata": {
        "id": "r9oI-KbRiGo1"
      },
      "source": [
        "# Часть 5: [Бонус] Анализ ошибок\n",
        "\n",
        "Проанализируйте твиты, которые были неправильно классифицированы вашей моделью. Как вы считаете, с чем связаны ошибки?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 340,
      "id": "1Rur4i_giGo1",
      "metadata": {
        "id": "1Rur4i_giGo1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Truth Predicted Tweet\n"
          ]
        }
      ],
      "source": [
        "print('Truth Predicted Tweet')\n",
        "for x, y in zip(test_x, test_y):\n",
        "    y_hat = naive_bayes_predict(x, logprior, loglikelihood)\n",
        "    # you code here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zaya-0lQiGo1",
      "metadata": {
        "id": "Zaya-0lQiGo1"
      },
      "source": [
        "# Часть 6: Предскажите тональность какого-нибудь интересного твита"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 341,
      "id": "M3Sk_Us3iGo1",
      "metadata": {
        "id": "M3Sk_Us3iGo1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "9.056120624523112\n"
          ]
        }
      ],
      "source": [
        "# Test with your own tweet - feel free to modify `my_tweet`\n",
        "my_tweet = 'I am happy because I am learning :)'\n",
        "\n",
        "p = naive_bayes_predict(my_tweet, logprior, loglikelihood)\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mexdMqm4iGo1",
      "metadata": {
        "id": "mexdMqm4iGo1"
      },
      "source": [
        "# Приведите краткий отчет по выполненному заданию"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "fe13ed55",
      "metadata": {},
      "source": [
        "В данной работе я ознакомился с библиотекой nltk. Понял работу наивного байесовского классификатора"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "3ce679d5bd49a012fff85ebfbef2e0bc9fa325516b42cbe6e67bd8cfdb383ae4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
